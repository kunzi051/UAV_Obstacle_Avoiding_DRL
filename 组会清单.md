问题整理

## 1 A* RRT ACO算法

#### 1.1 A*算法

启发式搜索算法，启发式函数f(n) = g(n) + h(n)，g(n)是从起始节点到当前节点的实际成本，是确定的；而h(n)是启发式函数估计的从当前节点到目标节点的成本，是不确定的，可能低估。

<img src="C:\Users\kunzi051\AppData\Roaming\Typora\typora-user-images\image-20240529101234000.png" alt="image-20240529101234000" style="zoom: 33%;" />

常用的H代价函数： 曼哈顿距离（适用于平面只能上下左右）；欧式距离；...

A算法在很多情况下是最优的，当启发式函数满足一致性（或称为单调性）时，A算法保证找到最短路径。

https://www.youtube.com/watch?v=i0x5fj4PqP4&t=16s

https://www.bilibili.com/video/BV1v44y1h7Dt/?spm_id_from=333.337.search-card.all.click&vd_source=8e89bab0a26ee05c70c9c27279dbc587

https://www.bilibili.com/video/BV16P411c7CL/?spm_id_from=333.337.search-card.all.click&vd_source=8e89bab0a26ee05c70c9c27279dbc587

#### 1.2 RRT算法

基本原理：

1. **初始化**：算法开始时，从起点创建一棵树，该树最初只包含一个节点，即起点。
2. **随机采样**：算法在配置空间中随机选择一个点。配置空间是描述机器人或移动体可以到达的所有位置的空间。
3. **最近邻搜索**：在当前树中找到距离随机采样点最近的节点，称为最近邻节点。
4. **扩展**：从最近邻节点向随机采样点“生长”一步。这一步的大小通常由一个最大步长参数决定，以确保移动的连续性和可行性。
5. **碰撞检测**：在生长过程中，算法需要检查新生成的路径段是否与环境中的障碍物发生碰撞。如果发生碰撞，则放弃这个路径段，并重新选择另一个随机点。
6. **连接**：如果新路径段没有与障碍物发生碰撞，则将随机采样点作为新节点添加到树中，并将其与最近邻节点连接起来。
7. **目标检测**：每次扩展后，算法都会检查新添加的节点是否在目标点的一个小的接受半径内。如果是，则算法成功找到一条到达目标的路径。
8. **迭代**：重复上述步骤，直到找到目标或达到某个终止条件（如最大迭代次数或时间限制）。
9. **路径平滑**：一旦找到到达目标的路径，可能还需要进行路径平滑处理，以优化路径的平滑度和连续性。

问题：

- **路径长度**：找到的路径可能不是最优的，可能比最短路径要长，即使放置更多的随机点，路径长度也不会改变（衍生了变体：**RRT*** 尝试减少路径长度。它通过比较新节点与树中其他节点的连接，如果重新连接可以减少路径长度，则进行更新。
- **随机性**：随机采样，结构不确定

https://www.youtube.com/watch?v=Ob3BIJkQJEw



#### 1.3 ACO算法

**基本思想：**

- 蚂蚁在寻找食物的过程中会释放一种名为信息素的化学物质，这种信息素能够吸引其他蚂蚁沿着相同的路径行走。
- 当一只蚂蚁找到食物后，它会沿着原路返回，并在路径上留下信息素。
- 其他蚂蚁会倾向于跟随信息素浓度较高的路径，因为这意味着它们更有可能找到食物。
- 随着时间的推移，信息素会逐渐挥发，导致那些较少使用的路径上的信息素浓度降低。



#### 1.4 重要参数信息（帮助分析算法对比结果）

A*：

- threshold=0.7 % 确定在扩展节点时考虑的邻居节点的最大步进距离 stop=threshold*1.5 % 停止扩展节点的条件，大于阈值的1.5倍
- 启发式函数：
- 1. 当前节点与下一个候选节点之间的距离 (`d1`)。
  2. 候选节点与目标节点之间的距离 (`d2`)。
  3. 启发式值 (`D`)，它通过距离计算得出，公式为 `50 / (d1 + d2)`。
  4. `H = D*flag； % flag为碰撞标志位，若碰撞则置为0`

RRT:

- stepSize=0.2 % 每次搜索时步进的大小，影响路径的平滑性和算法的探索精度 

- threshold=0.2 % 判定路径可达性的最大距离阈值，影响路径的连续性 

- maxFailedAttempts=10000 % 最大失败尝试次数，超过这个次数算法将终止 

- searchSize=1.1*[goal(1) - start(1), goal(2) - start(2), goal(3) - start(3)] % 搜索空间的大小，基于起点和终点的距离动态调整

- 随机选择延展目标（用于引导搜索过程）：策略为50-50

  算法在每一步随机选择一个目标点，要么是完全随机的一个点，要么是目标点（goal），各占50%的几率。

ACO：

- popNumber=50 % 蚁群中的蚂蚁数量，影响算法的搜索能力和多样性
- rou=0.1 % 信息素的蒸发因子，控制信息素随时间的衰减速率
- iterMax=80 % 最大迭代次数，影响算法的运行时间和搜索深度
- deltaX=0.2, deltaY=0.2, deltaZ=0.2 % 网格的分辨率，在各个方向上的步长
- gridXNumber=floor(abs(goal(1) - start(1)) / deltaX) % x轴方向上的网格数量
- gridYNumber=80, gridZNumber=80 % y轴和z轴方向上的网格数量
- ybegin=start(2) - 20*deltaY, zbegin=start(3) - 20*deltaZ % y和z轴的起始点偏移量
- ycMax=3, zcMax=3 % 蚂蚁在y轴和z轴方向上的最大移动范围
- pheromone=ones(gridXNumber, gridYNumber, gridZNumber) % 初始信息素矩阵，用于路径选择的概率
- maxFailedAttempts=10000 % 最大失败尝试次数，超过此次数算法终止
- searchSize=1.1*[goal(1) - start(1), goal(2) - start(2), goal(3) - start(3)] % 搜索空间的范围，基于起点和终点的距离动态调整



#### 1.5 算法分类

1. **启发式搜索（Heuristic Search）**：
   - 这类算法使用启发式函数来引导搜索过程，通常用于找到最短路径或最优解。
   - A*算法就是一种典型的启发式搜索算法，它通过评估每个节点的f值（f(n) = g(n) + h(n)），其中g(n)是实际到达成本，h(n)是启发式估计成本，来确定下一步最佳移动方向。
2. **采样（Sampling）**：
   - 采样基础的算法通过在搜索空间中随机或有目的地采样来探索可能的解。
   - RRT（Rapidly-exploring Random Tree）算法就是一种采样算法，它通过随机采样和局部规划逐步构建路径。
3. **人工势场法（Artificial Potential Fields, APF）**：
   - APF算法通过在搜索空间中定义吸引势和排斥势来引导移动体向目标移动。
   - 它不是纯粹的启发式搜索，因为它不依赖于启发式函数来评估路径的优劣，而是通过势场的变化来决定移动方向。
4. **元启发式算法（Metaheuristic Algorithms）**：
   - 这类算法是一类用于解决优化问题的高级策略，它们通常包括启发式规则和随机性来指导搜索过程。
   - ACO（Ant Colony Optimization）算法就是一种元启发式算法，它模拟蚂蚁觅食时留下信息素的行为，通过蚂蚁群体的协作来寻找最优路径。

在这些分类中，APF和ACO通常被视为元启发式算法，因为它们使用启发式信息和随机性来指导搜索过程，并且不局限于寻找最短路径，而是可以用于更广泛的优化问题。元启发式算法特别适用于复杂的、非线性的、或有多个局部最优解的问题，它们能够提供足够好的解，尽管这些解可能不是全局最优的。



## 2 results 

| 运行时间（s） | RL-APF     | A*      | RRT     | 蚁群     |      | 路径长度（km） | RL-APF     | A*      | RRT     | 蚁群     |
| ------------- | ---------- | ------- | ------- | -------- | ---- | -------------- | ---------- | ------- | ------- | -------- |
| 次数1         | 0.4009     | 65.8428 | 14.8065 | 113.6107 |      | 次数1          | 16.5919    | 16.2532 | 18.0700 | 29.4365  |
| 次数2         | 0.4390     | 67.2243 | 16.3362 | 110.7249 |      | 次数2          | 16.5919    | 16.2532 | 19.3942 | 29.7098  |
| 次数3         | 0.3719     | 69.8481 | 20.6897 | 120.8381 |      | 次数3          | 16.5919    | 16.2532 | 18.5390 | 29.3888  |
| 次数4         | 0.3039     | 68.4165 | 16.9163 | 127.1899 |      | 次数4          | 16.5919    | 16.2532 | 20.9289 | 29.3644  |
| 次数5         | 0.3379     | 70.0626 | 9.8442  | 113.8954 |      | 次数5          | 16.5919    | 16.2532 | 18.2516 | 28.0917  |
| 次数6         | 0.3479     | 65.0270 | 27.5004 | 127.4127 |      | 次数6          | 16.5919    | 16.2532 | 17.7441 | 29.8763  |
| 次数7         | 0.3429     | 73.4141 | 20.0573 | 119.9602 |      | 次数7          | 16.5919    | 16.2532 | 18.7547 | 27.3513  |
| 次数8         | 0.3259     | 71.2984 | 13.6994 | 114.6401 |      | 次数8          | 16.5919    | 16.2532 | 17.5327 | 29.8763  |
| 次数9         | 0.3300     | 68.2635 | 23.2173 | 113.8103 |      | 次数9          | 16.5919    | 16.2532 | 19.1838 | 29.2774  |
| 次数10        | 0.3369     | 66.3171 | 18.6629 | 127.3716 |      | 次数10         | 16.5919    | 16.2532 | 17.4832 | 27.8290  |
|               | 0.3537     | 68.5714 | 18.1730 | 118.9454 |      |                | 16.5919    | 16.2532 | 18.5882 | 29.0202  |
|               |            |         |         |          |      |                |            |         |         |          |
| **GS（°）**   | **RL-APF** | **A***  | **RRT** | **蚁群** |      | **LS（°）**    | **RL-APF** | **A***  | **RRT** | **蚁群** |
| 次数1         | 2.0282     | 11.6681 | 15.2600 | 56.8362  |      | 次数1          | 10.0000    | 48.6601 | 52.1800 | 107.5029 |
| 次数2         | 2.0282     | 11.6681 | 14.1178 | 51.3206  |      | 次数2          | 10.0000    | 48.6601 | 67.1706 | 104.7481 |
| 次数3         | 2.0282     | 11.6681 | 14.5792 | 47.5971  |      | 次数3          | 10.0000    | 48.6601 | 50.1801 | 99.4370  |
| 次数4         | 2.0282     | 11.6681 | 17.9658 | 56.3214  |      | 次数4          | 10.0000    | 48.6601 | 66.5399 | 110.9905 |
| 次数5         | 2.0282     | 11.6681 | 14.1691 | 49.0272  |      | 次数5          | 10.0000    | 48.6601 | 59.9022 | 107.5029 |
| 次数6         | 2.0282     | 11.6681 | 7.1880  | 47.7893  |      | 次数6          | 10.0000    | 48.6601 | 34.6235 | 97.1794  |
| 次数7         | 2.0282     | 11.6681 | 14.6201 | 48.1721  |      | 次数7          | 10.0000    | 48.6601 | 45.3134 | 113.2482 |
| 次数8         | 2.0282     | 11.6681 | 14.5477 | 47.7893  |      | 次数8          | 10.0000    | 48.6601 | 62.2645 | 97.1794  |
| 次数9         | 2.0282     | 11.6681 | 17.1799 | 51.9584  |      | 次数9          | 10.0000    | 48.6601 | 60.1135 | 102.1328 |
| 次数10        | 2.0282     | 11.6681 | 14.3001 | 47.6448  |      | 次数10         | 10.0000    | 48.6601 | 51.5943 | 95.7032  |
|               | 2.0282     | 11.6681 | 14.3928 | 50.4456  |      |                | 10.0000    | 48.6601 | 54.9882 | 103.5624 |

注：RL-APF和A*的长度 GS  LS为一次运行结果（多次运行，结果不变），RRT和ACO因为是概率性算法，每次随机rand，运行10次。                  



蚁群算法表现较差原因：

1. **算法适用性**：

   - 每种算法都有其适用的场景。ACO特别适合解决组合优化问题（TSP...），对于连续空间的路径规划问题，可能不是最佳选择。

2. **启发式信息不足**：

   - ACO依赖于信息素和启发式信息来引导蚂蚁的搜索过程。如果启发式信息不够有效或不足以指导搜索，可能导致路径不是最优的。

3. **参数调整**：

   - ACO算法的性能对参数（如信息素的蒸发率、蚂蚁数量、信息素的更新策略等）非常敏感。参数设置不当可能导致算法陷入局部最优或搜索效率低下。

4. **信息素更新策略**：

   - 如果信息素更新策略不够有效，可能会导致算法未能充分探索解空间，或者过分强化了某些并非最优的路径。

   

## 3 RL部分

#### 3.1 基础知识

强化学习通常包括以下几个关键概念：

1. **状态（State）**：智能体所处的环境状态。
2. **动作（Action）**：智能体在给定状态下可以采取的动作。
3. **奖励（Reward）**：智能体执行动作后获得的即时回报。
4. **策略（Policy）**：从状态到动作的映射，智能体根据策略选择动作。
5. **价值函数（Value Function）**：预测采取某个策略后获得的累积奖励。
6. **模型（Model）**：智能体对环境的理解，预测执行动作后状态和奖励的变化。

强化学习的方法可以分为几种类型，包括：

- **基于模型的方法**：智能体尝试学习环境的模型，然后使用这个模型来预测未来的状态和奖励。
- **无模型的方法**：智能体不尝试学习环境的模型，而是直接从经验中学习策略和价值函数。
- **基于价值的方法**：如Q学习（Q-Learning）和SARSA，它们直接学习价值函数。
- **基于策略的方法**：如策略梯度方法，它们直接学习策略。



#### 3.2 APF与RL的输入/输出/参数联系

1. 猝死急救无人机在三维空间中某一点接收到障碍物的相对位置 以及目标的相对位置。
2. 将其输入到对应障碍物的强化学习策略网络，即可得到对应障碍物在当前环境状态下决策出的斥力增益系数`ηi`
3. 重复这个步骤，即可在`t`时刻得到所有障碍物的斥力增益系数向量`η`  。
4. 将该向量作为强化学习的动作，输入规划层，规划层计算得到机器人的下一航路点，并返回从状态`st`到`s(t+1)`的奖励值  。
5. 算法将`(st,at,rt,s(t+1))`存储到对应障碍物的数据缓存中，以便模型后续学习。





